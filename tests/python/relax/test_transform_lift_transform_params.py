# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import tvm
from tvm import relax
from tvm.script import relax as R, tir as T
import numpy as np


def test_transform_params():
    """Demonstrate how to compile and optimize with parameters not available at compile time."""

    @tvm.script.ir_module
    class Module:
        @T.prim_func
        def transform_layout_IOHW_to_OIHW(
            w1: T.Buffer((3, 16, 3, 3), "float32"), out: T.Buffer((16, 3, 3, 3), "float32")
        ) -> None:
            for ax0, ax1, ax2, ax3 in T.grid(16, 3, 3, 3):
                with T.block("layout_transform"):
                    o, i, h, w = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                    out[o, i, h, w] = w1[i, o, h, w]

        @R.function
        def main(
            x: R.Tensor((1, 3, 224, 224), "float32"), w1: R.Tensor((3, 16, 3, 3), "float32")
        ) -> R.Tensor((1, 16, 224, 224), "float32"):
            R.func_attr(
                {"param_begin": 1, "param_end": 2}
            )  # annotate the tensors that are parameters, [begin, end] are range of indices of the function params
            with R.dataflow():
                w1_transformed = R.call_tir(
                    transform_layout_IOHW_to_OIHW, w1, R.Tensor((16, 3, 3, 3), "float32")
                )  # this is weight transformation generated by passes like RewriteWeightLayout
                conv1 = R.nn.conv2d(
                    x, w1_transformed, padding=(1, 1), data_layout="NCHW", kernel_layout="OIHW"
                )
                R.output(conv1)
            return conv1

    @tvm.script.ir_module
    class AfterLiftTransformParams:
        @T.prim_func
        def transform_layout_IOHW_to_OIHW(
            w1: T.Buffer((3, 16, 3, 3), "float32"), out: T.Buffer((16, 3, 3, 3), "float32")
        ) -> None:
            for ax0, ax1, ax2, ax3 in T.grid(16, 3, 3, 3):
                with T.block("layout_transform"):
                    o, i, h, w = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                    out[o, i, h, w] = w1[i, o, h, w]

        @R.function
        def transform_params(
            params: R.Tuple((R.Tensor((3, 16, 3, 3), "float32"),))
        ) -> R.Tuple((R.Tensor((16, 3, 3, 3), "float32"),)):
            with R.dataflow():
                w1 = params[0]
                w1_transformed = R.call_tir(
                    transform_layout_IOHW_to_OIHW, w1, R.Tensor((16, 3, 3, 3), "float32")
                )
                tparams = (w1_transformed,)
                R.output(tparams)
            return tparams

        @R.function
        def main(
            x: R.Tensor((1, 3, 224, 224), "float32"),
            params: R.Tuple(
                R.Tensor((16, 3, 3, 3), "float32"),
            ),
        ) -> R.Tensor((1, 16, 224, 224), "float32"):
            # R.func_attr({})
            with R.dataflow():
                w1 = params[0]
                conv1 = R.nn.conv2d(x, w1, padding=(1, 1), data_layout="NCHW", kernel_layout="OIHW")
                R.output(conv1)
            return conv1

    mod = Module
    target = tvm.target.Target("llvm")
    dev = tvm.cpu(0)

    # Build the VM.
    # Step 1: Lift the params transformation to a separate function that can be called at runtime. We can run optimizations like FoldScaleAxis, RewriteWeightLayout before this pass.
    with tvm.transform.PassContext(opt_level=1):
        seq = tvm.transform.Sequential([relax.transform.LiftTransformParams()])
        mod = seq(mod)
    tvm.ir.assert_structural_equal(mod, AfterLiftTransformParams)

    # Step 2: Run the normal compilation workflow
    mod = relax.transform.LegalizeOps()(mod)
    exec = relax.vm.build(mod, target, params=None)  # optimize and compile the model without params
    vm = relax.vm.VirtualMachine(exec, dev)

    x = tvm.nd.array(np.random.uniform(size=(1, 3, 224, 224)).astype(dtype="float32"), dev)

    # these weights are only available at runtime
    w1 = tvm.nd.array(np.random.uniform(size=(3, 16, 3, 3)).astype(dtype="float32"), dev)
    params = (w1,)

    # do runtime weight transformation
    tparams = vm["transform_params"](params)

    # run the optimized model with transformed weights
    out = vm["main"](x, tparams)


if __name__ == "__main__":
    test_transform_params()
